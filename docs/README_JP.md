<!-- mcp-name: io.github.D4Vinci/Scrapling -->

<h1 align="center">
    <a href="https://scrapling.readthedocs.io">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/docs/assets/cover_dark.svg?sanitize=true">
          <img alt="Scrapling Poster" src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/docs/assets/cover_light.svg?sanitize=true">
        </picture>
    </a>
    <br>
    <small>Effortless Web Scraping for the Modern Web</small>
</h1>

<p align="center">
    <a href="https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml" alt="Tests">
        <img alt="Tests" src="https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml/badge.svg"></a>
    <a href="https://badge.fury.io/py/Scrapling" alt="PyPI version">
        <img alt="PyPI version" src="https://badge.fury.io/py/Scrapling.svg"></a>
    <a href="https://pepy.tech/project/scrapling" alt="PyPI Downloads">
        <img alt="PyPI Downloads" src="https://static.pepy.tech/personalized-badge/scrapling?period=total&units=INTERNATIONAL_SYSTEM&left_color=GREY&right_color=GREEN&left_text=Downloads"></a>
    <br/>
    <a href="https://discord.gg/EMgGbDceNQ" alt="Discord" target="_blank">
      <img alt="Discord" src="https://img.shields.io/discord/1360786381042880532?style=social&logo=discord&link=https%3A%2F%2Fdiscord.gg%2FEMgGbDceNQ">
    </a>
    <a href="https://x.com/Scrapling_dev" alt="X (formerly Twitter)">
      <img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/Scrapling_dev?style=social&logo=x&link=https%3A%2F%2Fx.com%2FScrapling_dev">
    </a>
    <br/>
    <a href="https://pypi.org/project/scrapling/" alt="Supported Python versions">
        <img alt="Supported Python versions" src="https://img.shields.io/pypi/pyversions/scrapling.svg"></a>
</p>

<p align="center">
    <a href="https://scrapling.readthedocs.io/en/latest/parsing/selection/"><strong>é¸æŠãƒ¡ã‚½ãƒƒãƒ‰</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/fetching/choosing/"><strong>Fetcherã®é¸ã³æ–¹</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/spiders/architecture.html"><strong>ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/spiders/proxy-blocking.html"><strong>ãƒ—ãƒ­ã‚­ã‚·ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/cli/overview/"><strong>CLI</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/ai/mcp-server/"><strong>MCPãƒ¢ãƒ¼ãƒ‰</strong></a>
</p>

Scraplingã¯ã€å˜ä¸€ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰æœ¬æ ¼çš„ãªã‚¯ãƒ­ãƒ¼ãƒ«ã¾ã§ã™ã¹ã¦ã‚’å‡¦ç†ã™ã‚‹é©å¿œå‹Web Scrapingãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚

ãã®ãƒ‘ãƒ¼ã‚µãƒ¼ã¯ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®å¤‰æ›´ã‹ã‚‰å­¦ç¿’ã—ã€ãƒšãƒ¼ã‚¸ãŒæ›´æ–°ã•ã‚ŒãŸã¨ãã«è¦ç´ ã‚’è‡ªå‹•çš„ã«å†é…ç½®ã—ã¾ã™ã€‚Fetcherã¯ã™ãã«ä½¿ãˆã‚‹Cloudflare Turnstileãªã©ã®ã‚¢ãƒ³ãƒãƒœãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚’å›é¿ã—ã¾ã™ã€‚ãã—ã¦Spiderãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚Šã€Pause & Resumeã‚„è‡ªå‹•Proxyå›è»¢æ©Ÿèƒ½ã‚’å‚™ãˆãŸä¸¦è¡Œãƒãƒ«ãƒSessionã‚¯ãƒ­ãƒ¼ãƒ«ã¸ã¨ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã§ãã¾ã™ â€” ã™ã¹ã¦ã‚ãšã‹æ•°è¡Œã®Pythonã§ã€‚1ã¤ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€å¦¥å”ãªã—ã€‚

ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆã¨Streamingã«ã‚ˆã‚‹è¶…é«˜é€Ÿã‚¯ãƒ­ãƒ¼ãƒ«ã€‚Web Scraperã«ã‚ˆã£ã¦ã€Web Scraperã¨ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãŸã‚ã«æ§‹ç¯‰ã•ã‚Œã€èª°ã«ã§ã‚‚ä½•ã‹ãŒã‚ã‚Šã¾ã™ã€‚

```python
from scrapling.fetchers import Fetcher, AsyncFetcher, StealthyFetcher, DynamicFetcher
StealthyFetcher.adaptive = True
p = StealthyFetcher.fetch('https://example.com', headless=True, network_idle=True)  # ãƒ¬ãƒ¼ãƒ€ãƒ¼ã®ä¸‹ã§ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚’å–å¾—ï¼
products = p.css('.product', auto_save=True)                                        # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®ãƒ‡ã‚¶ã‚¤ãƒ³å¤‰æ›´ã«è€ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ï¼
products = p.css('.product', adaptive=True)                                         # å¾Œã§ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®æ§‹é€ ãŒå¤‰ã‚ã£ãŸã‚‰ã€`adaptive=True`ã‚’æ¸¡ã—ã¦è¦‹ã¤ã‘ã‚‹ï¼
```
ã¾ãŸã¯æœ¬æ ¼çš„ãªã‚¯ãƒ­ãƒ¼ãƒ«ã¸ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
```python
from scrapling.spiders import Spider, Response

class MySpider(Spider):
  name = "demo"
  start_urls = ["https://example.com/"]

  async def parse(self, response: Response):
      for item in response.css('.product'):
          yield {"title": item.css('h2::text').get()}

MySpider().start()
```


# ãƒ—ãƒ©ãƒãƒŠã‚¹ãƒãƒ³ã‚µãƒ¼

<i><sub>ã“ã“ã«æœ€åˆã«è¡¨ç¤ºã•ã‚Œã‚‹ä¼æ¥­ã«ãªã‚Šã¾ã›ã‚“ã‹ï¼Ÿ[ã“ã¡ã‚‰](https://github.com/sponsors/D4Vinci/sponsorships?tier_id=586646)ã‚’ã‚¯ãƒªãƒƒã‚¯</sub></i>
# ã‚¹ãƒãƒ³ã‚µãƒ¼

<!-- sponsors -->

<a href="https://www.thordata.com/?ls=github&lk=github" target="_blank" title="Unblockable proxies and scraping infrastructure, delivering real-time, reliable web data to power AI models and workflows."><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/thordata.jpg"></a>
<a href="https://evomi.com?utm_source=github&utm_medium=banner&utm_campaign=d4vinci-scrapling" target="_blank" title="Evomi is your Swiss Quality Proxy Provider, starting at $0.49/GB"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/evomi.png"></a>
<a href="https://serpapi.com/?utm_source=scrapling" target="_blank" title="Scrape Google and other search engines with SerpApi"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/SerpApi.png"></a>
<a href="https://visit.decodo.com/Dy6W0b" target="_blank" title="Try the Most Efficient Residential Proxies for Free"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/decodo.png"></a>
<a href="https://petrosky.io/d4vinci" target="_blank" title="PetroSky delivers cutting-edge VPS hosting."><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/petrosky.png"></a>
<a href="https://hasdata.com/?utm_source=github&utm_medium=banner&utm_campaign=D4Vinci" target="_blank" title="The web scraping service that actually beats anti-bot systems!"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/hasdata.png"></a>
<a href="https://proxyempire.io/?ref=scrapling&utm_source=scrapling" target="_blank" title="Collect The Data Your Project Needs with the Best Residential Proxies"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/ProxyEmpire.png"></a>
<a href="https://hypersolutions.co/?utm_source=github&utm_medium=readme&utm_campaign=scrapling" target="_blank" title="Bot Protection Bypass API for Akamai, DataDome, Incapsula & Kasada"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/HyperSolutions.png"></a>


<a href="https://www.swiftproxy.net/" target="_blank" title="Unlock Reliable Proxy Services with Swiftproxy!"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/swiftproxy.png"></a>
<a href="https://www.rapidproxy.io/?ref=d4v" target="_blank" title="Affordable Access to the Proxy World â€“ bypass CAPTCHAs blocks, and avoid additional costs."><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/rapidproxy.jpg"></a>
<a href="https://browser.cash/?utm_source=D4Vinci&utm_medium=referral" target="_blank" title="Browser Automation & AI Browser Agent Platform"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/browserCash.png"></a>

<!-- /sponsors -->

<i><sub>ã“ã“ã«åºƒå‘Šã‚’è¡¨ç¤ºã—ãŸã„ã§ã™ã‹ï¼Ÿ[ã“ã¡ã‚‰](https://github.com/sponsors/D4Vinci)ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ã‚ãªãŸã«åˆã£ãŸãƒ†ã‚£ã‚¢ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼</sub></i>

---

## ä¸»ãªæ©Ÿèƒ½

### Spider â€” æœ¬æ ¼çš„ãªã‚¯ãƒ­ãƒ¼ãƒ«ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- ğŸ•·ï¸ **Scrapyé¢¨ã®Spider API**ï¼š`start_urls`ã€async `parse` callbackã€`Request`/`Response`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§Spiderã‚’å®šç¾©ã€‚
- âš¡ **ä¸¦è¡Œã‚¯ãƒ­ãƒ¼ãƒ«**ï¼šè¨­å®šå¯èƒ½ãªä¸¦è¡Œæ•°åˆ¶é™ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ã”ã¨ã®ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é…å»¶ã€‚
- ğŸ”„ **ãƒãƒ«ãƒSessionã‚µãƒãƒ¼ãƒˆ**ï¼šHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ã‚¹ãƒ†ãƒ«ã‚¹ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ â€” IDã«ã‚ˆã£ã¦ç•°ãªã‚‹Sessionã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€‚
- ğŸ’¾ **Pause & Resume**ï¼šCheckpointãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ­ãƒ¼ãƒ«æ°¸ç¶šåŒ–ã€‚Ctrl+Cã§æ­£å¸¸ã«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ï¼›å†èµ·å‹•ã™ã‚‹ã¨ä¸­æ–­ã—ãŸã¨ã“ã‚ã‹ã‚‰å†é–‹ã€‚
- ğŸ“¡ **Streamingãƒ¢ãƒ¼ãƒ‰**ï¼š`async for item in spider.stream()`ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆã¨ã¨ã‚‚ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’Streamingã§å—ä¿¡ â€” UIã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€é•·æ™‚é–“å®Ÿè¡Œã‚¯ãƒ­ãƒ¼ãƒ«ã«æœ€é©ã€‚
- ğŸ›¡ï¸ **ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æ¤œå‡º**ï¼šã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªãƒ­ã‚¸ãƒƒã‚¯ã«ã‚ˆã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã®è‡ªå‹•æ¤œå‡ºã¨ãƒªãƒˆãƒ©ã‚¤ã€‚
- ğŸ“¦ **çµ„ã¿è¾¼ã¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ**ï¼šãƒ•ãƒƒã‚¯ã‚„ç‹¬è‡ªã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã¾ãŸã¯çµ„ã¿è¾¼ã¿ã®JSON/JSONLã§çµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€‚ãã‚Œãã‚Œ`result.items.to_json()` / `result.items.to_jsonl()`ã‚’ä½¿ç”¨ã€‚

### Sessionã‚µãƒãƒ¼ãƒˆä»˜ãé«˜åº¦ãªã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆå–å¾—
- **HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ**ï¼š`Fetcher`ã‚¯ãƒ©ã‚¹ã§é«˜é€Ÿã‹ã¤ã‚¹ãƒ†ãƒ«ã‚¹ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®TLS fingerprintã€ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¨¡å€£ã—ã€HTTP/3ã‚’ä½¿ç”¨å¯èƒ½ã€‚
- **å‹•çš„èª­ã¿è¾¼ã¿**ï¼šPlaywrightã®Chromiumã¨Google Chromeã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹`DynamicFetcher`ã‚¯ãƒ©ã‚¹ã«ã‚ˆã‚‹å®Œå…¨ãªãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã§å‹•çš„ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚’å–å¾—ã€‚
- **ã‚¢ãƒ³ãƒãƒœãƒƒãƒˆå›é¿**ï¼š`StealthyFetcher`ã¨fingerprintå½è£…ã«ã‚ˆã‚‹é«˜åº¦ãªã‚¹ãƒ†ãƒ«ã‚¹æ©Ÿèƒ½ã€‚è‡ªå‹•åŒ–ã§Cloudflareã®Turnstile/Interstitialã®ã™ã¹ã¦ã®ã‚¿ã‚¤ãƒ—ã‚’ç°¡å˜ã«å›é¿ã€‚
- **Sessionç®¡ç†**ï¼šãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã§Cookieã¨çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã®`FetcherSession`ã€`StealthySession`ã€`DynamicSession`ã‚¯ãƒ©ã‚¹ã«ã‚ˆã‚‹æ°¸ç¶šçš„ãªSessionã‚µãƒãƒ¼ãƒˆã€‚
- **Proxyå›è»¢**ï¼šã™ã¹ã¦ã®Sessionã‚¿ã‚¤ãƒ—ã«å¯¾å¿œã—ãŸãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³ã¾ãŸã¯ã‚«ã‚¹ã‚¿ãƒ æˆ¦ç•¥ã®çµ„ã¿è¾¼ã¿`ProxyRotator`ã€ã•ã‚‰ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã®Proxyã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã€‚
- **ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ–ãƒ­ãƒƒã‚¯**ï¼šãƒ–ãƒ©ã‚¦ã‚¶ãƒ™ãƒ¼ã‚¹ã®Fetcherã§ç‰¹å®šã®ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆãŠã‚ˆã³ãã®ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼‰ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒ–ãƒ­ãƒƒã‚¯ã€‚
- **asyncã‚µãƒãƒ¼ãƒˆ**ï¼šã™ã¹ã¦ã®FetcherãŠã‚ˆã³å°‚ç”¨asyncSessionã‚¯ãƒ©ã‚¹å…¨ä½“ã§ã®å®Œå…¨ãªasyncã‚µãƒãƒ¼ãƒˆã€‚

### é©å¿œå‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨AIçµ±åˆ
- ğŸ”„ **ã‚¹ãƒãƒ¼ãƒˆè¦ç´ è¿½è·¡**ï¼šã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªé¡ä¼¼æ€§ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®å¤‰æ›´å¾Œã«è¦ç´ ã‚’å†é…ç½®ã€‚
- ğŸ¯ **ã‚¹ãƒãƒ¼ãƒˆæŸ”è»Ÿé¸æŠ**ï¼šCSSã‚»ãƒ¬ã‚¯ã‚¿ã€XPathã‚»ãƒ¬ã‚¯ã‚¿ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã€ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã€æ­£è¦è¡¨ç¾æ¤œç´¢ãªã©ã€‚
- ğŸ” **é¡ä¼¼è¦ç´ ã®æ¤œå‡º**ï¼šè¦‹ã¤ã‹ã£ãŸè¦ç´ ã«é¡ä¼¼ã—ãŸè¦ç´ ã‚’è‡ªå‹•çš„ã«ç‰¹å®šã€‚
- ğŸ¤– **AIã¨ä½¿ç”¨ã™ã‚‹MCPã‚µãƒ¼ãƒãƒ¼**ï¼šAIæ”¯æ´Web Scrapingã¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®ãŸã‚ã®çµ„ã¿è¾¼ã¿MCPã‚µãƒ¼ãƒãƒ¼ã€‚MCPã‚µãƒ¼ãƒãƒ¼ã¯ã€AIï¼ˆClaude/Cursorãªã©ï¼‰ã«æ¸¡ã™å‰ã«Scraplingã‚’æ´»ç”¨ã—ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹å¼·åŠ›ã§ã‚«ã‚¹ã‚¿ãƒ ãªæ©Ÿèƒ½ã‚’å‚™ãˆã¦ãŠã‚Šã€æ“ä½œã‚’é«˜é€ŸåŒ–ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã“ã¨ã§ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã¾ã™ã€‚ï¼ˆ[ãƒ‡ãƒ¢å‹•ç”»](https://www.youtube.com/watch?v=qyFk3ZNwOxE)ï¼‰

### é«˜æ€§èƒ½ã§å®Ÿæˆ¦ãƒ†ã‚¹ãƒˆæ¸ˆã¿ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- ğŸš€ **è¶…é«˜é€Ÿ**ï¼šã»ã¨ã‚“ã©ã®Pythonã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä¸Šå›ã‚‹æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‚
- ğŸ”‹ **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**ï¼šæœ€å°ã®ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆã®ãŸã‚ã®æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨é…å»¶èª­ã¿è¾¼ã¿ã€‚
- âš¡ **é«˜é€ŸJSONã‚·ãƒªã‚¢ãƒ«åŒ–**ï¼šæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®10å€ã®é€Ÿåº¦ã€‚
- ğŸ—ï¸ **å®Ÿæˆ¦ãƒ†ã‚¹ãƒˆæ¸ˆã¿**ï¼šScraplingã¯92%ã®ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã¨å®Œå…¨ãªå‹ãƒ’ãƒ³ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’å‚™ãˆã¦ã„ã‚‹ã ã‘ã§ãªãã€éå»1å¹´é–“ã«æ•°ç™¾äººã®Web Scraperã«ã‚ˆã£ã¦æ¯æ—¥ä½¿ç”¨ã•ã‚Œã¦ãã¾ã—ãŸã€‚

### é–‹ç™ºè€…/Web Scraperã«ã‚„ã•ã—ã„ä½“é¨“
- ğŸ¯ **ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–Web Scraping Shell**ï¼šScraplingçµ±åˆã€ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã€curlãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’Scraplingãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¤‰æ›ã—ãŸã‚Šã€ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆçµæœã‚’è¡¨ç¤ºã—ãŸã‚Šã™ã‚‹ãªã©ã®æ–°ã—ã„ãƒ„ãƒ¼ãƒ«ã‚’å‚™ãˆãŸã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®çµ„ã¿è¾¼ã¿IPython Shellã§ã€Web Scrapingã‚¹ã‚¯ãƒªãƒ—ãƒˆã®é–‹ç™ºã‚’åŠ é€Ÿã€‚
- ğŸš€ **ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰ç›´æ¥ä½¿ç”¨**ï¼šã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã€ã‚³ãƒ¼ãƒ‰ã‚’ä¸€è¡Œã‚‚æ›¸ã‹ãšã«Scraplingã‚’ä½¿ç”¨ã—ã¦URLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ãã¾ã™ï¼
- ğŸ› ï¸ **è±Šå¯ŒãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³API**ï¼šè¦ªã€å…„å¼Ÿã€å­ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰ã«ã‚ˆã‚‹é«˜åº¦ãªDOMãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«ã€‚
- ğŸ§¬ **å¼·åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†**ï¼šçµ„ã¿è¾¼ã¿ã®æ­£è¦è¡¨ç¾ã€ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ã‚½ãƒƒãƒ‰ã€æœ€é©åŒ–ã•ã‚ŒãŸæ–‡å­—åˆ—æ“ä½œã€‚
- ğŸ“ **è‡ªå‹•ã‚»ãƒ¬ã‚¯ã‚¿ç”Ÿæˆ**ï¼šä»»æ„ã®è¦ç´ ã«å¯¾ã—ã¦å …ç‰¢ãªCSS/XPathã‚»ãƒ¬ã‚¯ã‚¿ã‚’ç”Ÿæˆã€‚
- ğŸ”Œ **é¦´æŸ“ã¿ã®ã‚ã‚‹API**ï¼šScrapy/Parselã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹åŒã˜ç–‘ä¼¼è¦ç´ ã‚’æŒã¤Scrapy/BeautifulSoupã«ä¼¼ãŸè¨­è¨ˆã€‚
- ğŸ“˜ **å®Œå…¨ãªå‹ã‚«ãƒãƒ¬ãƒƒã‚¸**ï¼šå„ªã‚ŒãŸIDEã‚µãƒãƒ¼ãƒˆã¨ã‚³ãƒ¼ãƒ‰è£œå®Œã®ãŸã‚ã®å®Œå…¨ãªå‹ãƒ’ãƒ³ãƒˆã€‚ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å…¨ä½“ãŒå¤‰æ›´ã®ãŸã³ã«**PyRight**ã¨**MyPy**ã§è‡ªå‹•çš„ã«ã‚¹ã‚­ãƒ£ãƒ³ã•ã‚Œã¾ã™ã€‚
- ğŸ”‹ **ã™ãã«ä½¿ãˆã‚‹Dockerã‚¤ãƒ¡ãƒ¼ã‚¸**ï¼šå„ãƒªãƒªãƒ¼ã‚¹ã§ã€ã™ã¹ã¦ã®ãƒ–ãƒ©ã‚¦ã‚¶ã‚’å«ã‚€Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ãŒè‡ªå‹•çš„ã«ãƒ“ãƒ«ãƒ‰ãŠã‚ˆã³ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚

## ã¯ã˜ã‚ã«

æ·±ãæ˜ã‚Šä¸‹ã’ãšã«ã€Scraplingã«ã§ãã‚‹ã“ã¨ã®ç°¡å˜ãªæ¦‚è¦ã‚’ãŠè¦‹ã›ã—ã¾ã—ã‚‡ã†ã€‚

### åŸºæœ¬çš„ãªä½¿ã„æ–¹
Sessionã‚µãƒãƒ¼ãƒˆä»˜ãHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ
```python
from scrapling.fetchers import Fetcher, FetcherSession

with FetcherSession(impersonate='chrome') as session:  # Chromeã®TLS fingerprintã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨
    page = session.get('https://quotes.toscrape.com/', stealthy_headers=True)
    quotes = page.css('.quote .text::text').getall()

# ã¾ãŸã¯ä¸€å›é™ã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½¿ç”¨
page = Fetcher.get('https://quotes.toscrape.com/')
quotes = page.css('.quote .text::text').getall()
```
é«˜åº¦ãªã‚¹ãƒ†ãƒ«ã‚¹ãƒ¢ãƒ¼ãƒ‰
```python
from scrapling.fetchers import StealthyFetcher, StealthySession

with StealthySession(headless=True, solve_cloudflare=True) as session:  # å®Œäº†ã™ã‚‹ã¾ã§ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ã„ãŸã¾ã¾ã«ã™ã‚‹
    page = session.fetch('https://nopecha.com/demo/cloudflare', google_search=False)
    data = page.css('#padded_content a').getall()

# ã¾ãŸã¯ä¸€å›é™ã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã€ã“ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãŸã‚ã«ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ãã€å®Œäº†å¾Œã«é–‰ã˜ã‚‹
page = StealthyFetcher.fetch('https://nopecha.com/demo/cloudflare')
data = page.css('#padded_content a').getall()
```
å®Œå…¨ãªãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–
```python
from scrapling.fetchers import DynamicFetcher, DynamicSession

with DynamicSession(headless=True, disable_resources=False, network_idle=True) as session:  # å®Œäº†ã™ã‚‹ã¾ã§ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ã„ãŸã¾ã¾ã«ã™ã‚‹
    page = session.fetch('https://quotes.toscrape.com/', load_dom=False)
    data = page.xpath('//span[@class="text"]/text()').getall()  # ãŠå¥½ã¿ã§ã‚ã‚Œã°XPathã‚»ãƒ¬ã‚¯ã‚¿ã‚’ä½¿ç”¨

# ã¾ãŸã¯ä¸€å›é™ã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã€ã“ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãŸã‚ã«ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ãã€å®Œäº†å¾Œã«é–‰ã˜ã‚‹
page = DynamicFetcher.fetch('https://quotes.toscrape.com/')
data = page.css('.quote .text::text').getall()
```

### Spider
ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆã€è¤‡æ•°ã®Sessionã‚¿ã‚¤ãƒ—ã€Pause & Resumeã‚’å‚™ãˆãŸæœ¬æ ¼çš„ãªã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’æ§‹ç¯‰ï¼š
```python
from scrapling.spiders import Spider, Request, Response

class QuotesSpider(Spider):
    name = "quotes"
    start_urls = ["https://quotes.toscrape.com/"]
    concurrent_requests = 10

    async def parse(self, response: Response):
        for quote in response.css('.quote'):
            yield {
                "text": quote.css('.text::text').get(),
                "author": quote.css('.author::text').get(),
            }

        next_page = response.css('.next a')
        if next_page:
            yield response.follow(next_page[0].attrib['href'])

result = QuotesSpider().start()
print(f"{len(result.items)}ä»¶ã®å¼•ç”¨ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ã¾ã—ãŸ")
result.items.to_json("quotes.json")
```
å˜ä¸€ã®Spiderã§è¤‡æ•°ã®Sessionã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨ï¼š
```python
from scrapling.spiders import Spider, Request, Response
from scrapling.fetchers import FetcherSession, AsyncStealthySession

class MultiSessionSpider(Spider):
    name = "multi"
    start_urls = ["https://example.com/"]

    def configure_sessions(self, manager):
        manager.add("fast", FetcherSession(impersonate="chrome"))
        manager.add("stealth", AsyncStealthySession(headless=True), lazy=True)

    async def parse(self, response: Response):
        for link in response.css('a::attr(href)').getall():
            # ä¿è­·ã•ã‚ŒãŸãƒšãƒ¼ã‚¸ã¯ã‚¹ãƒ†ãƒ«ã‚¹Sessionã‚’é€šã—ã¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
            if "protected" in link:
                yield Request(link, sid="stealth")
            else:
                yield Request(link, sid="fast", callback=self.parse)  # æ˜ç¤ºçš„ãªcallback
```
Checkpointã‚’ä½¿ç”¨ã—ã¦é•·æ™‚é–“ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’Pause & Resumeï¼š
```python
QuotesSpider(crawldir="./crawl_data").start()
```
Ctrl+Cã‚’æŠ¼ã™ã¨æ­£å¸¸ã«ä¸€æ™‚åœæ­¢ã—ã€é€²æ—ã¯è‡ªå‹•çš„ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚å¾Œã§Spiderã‚’å†åº¦èµ·å‹•ã™ã‚‹éš›ã«åŒã˜`crawldir`ã‚’æ¸¡ã™ã¨ã€ä¸­æ–­ã—ãŸã¨ã“ã‚ã‹ã‚‰å†é–‹ã—ã¾ã™ã€‚

### é«˜åº¦ãªãƒ‘ãƒ¼ã‚¹ã¨ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
```python
from scrapling.fetchers import Fetcher

# è±Šå¯Œãªè¦ç´ é¸æŠã¨ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
page = Fetcher.get('https://quotes.toscrape.com/')

# è¤‡æ•°ã®é¸æŠãƒ¡ã‚½ãƒƒãƒ‰ã§å¼•ç”¨ã‚’å–å¾—
quotes = page.css('.quote')  # CSSã‚»ãƒ¬ã‚¯ã‚¿
quotes = page.xpath('//div[@class="quote"]')  # XPath
quotes = page.find_all('div', {'class': 'quote'})  # BeautifulSoupã‚¹ã‚¿ã‚¤ãƒ«
# ä»¥ä¸‹ã¨åŒã˜
quotes = page.find_all('div', class_='quote')
quotes = page.find_all(['div'], class_='quote')
quotes = page.find_all(class_='quote')  # ãªã©...
# ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã§è¦ç´ ã‚’æ¤œç´¢
quotes = page.find_by_text('quote', tag='div')

# é«˜åº¦ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
quote_text = page.css('.quote')[0].css('.text::text').get()
quote_text = page.css('.quote').css('.text::text').getall()  # ãƒã‚§ãƒ¼ãƒ³ã‚»ãƒ¬ã‚¯ã‚¿
first_quote = page.css('.quote')[0]
author = first_quote.next_sibling.css('.author::text')
parent_container = first_quote.parent

# è¦ç´ ã®é–¢é€£æ€§ã¨é¡ä¼¼æ€§
similar_elements = first_quote.find_similar()
below_elements = first_quote.below_elements()
```
ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚’å–å¾—ã›ãšã«ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ã™ãã«ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼š
```python
from scrapling.parser import Selector

page = Selector("<html>...</html>")
```
ã¾ã£ãŸãåŒã˜æ–¹æ³•ã§å‹•ä½œã—ã¾ã™ï¼

### éåŒæœŸSessionç®¡ç†ã®ä¾‹
```python
import asyncio
from scrapling.fetchers import FetcherSession, AsyncStealthySession, AsyncDynamicSession

async with FetcherSession(http3=True) as session:  # `FetcherSession`ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¢ã‚¦ã‚§ã‚¢ã§ã€åŒæœŸ/éåŒæœŸä¸¡æ–¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å‹•ä½œå¯èƒ½
    page1 = session.get('https://quotes.toscrape.com/')
    page2 = session.get('https://quotes.toscrape.com/', impersonate='firefox135')

# éåŒæœŸSessionã®ä½¿ç”¨
async with AsyncStealthySession(max_pages=2) as session:
    tasks = []
    urls = ['https://example.com/page1', 'https://example.com/page2']

    for url in urls:
        task = session.fetch(url)
        tasks.append(task)

    print(session.get_pool_stats())  # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ - ãƒ–ãƒ©ã‚¦ã‚¶ã‚¿ãƒ–ãƒ—ãƒ¼ãƒ«ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆãƒ“ã‚¸ãƒ¼/ãƒ•ãƒªãƒ¼/ã‚¨ãƒ©ãƒ¼ï¼‰
    results = await asyncio.gather(*tasks)
    print(session.get_pool_stats())
```

## CLIã¨ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–Shell

Scraplingã«ã¯å¼·åŠ›ãªã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼š

[![asciicast](https://asciinema.org/a/736339.svg)](https://asciinema.org/a/736339)

ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–Web Scraping Shellã‚’èµ·å‹•
```bash
scrapling shell
```
ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã›ãšã«ç›´æ¥ãƒšãƒ¼ã‚¸ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æŠ½å‡ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§`body`ã‚¿ã‚°å†…ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼‰ã€‚å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒ`.txt`ã§çµ‚ã‚ã‚‹å ´åˆã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæŠ½å‡ºã•ã‚Œã¾ã™ã€‚`.md`ã§çµ‚ã‚ã‚‹å ´åˆã€HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®Markdownè¡¨ç¾ã«ãªã‚Šã¾ã™ã€‚`.html`ã§çµ‚ã‚ã‚‹å ´åˆã€HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãã®ã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚
```bash
scrapling extract get 'https://example.com' content.md
scrapling extract get 'https://example.com' content.txt --css-selector '#fromSkipToProducts' --impersonate 'chrome'  # CSSã‚»ãƒ¬ã‚¯ã‚¿'#fromSkipToProducts'ã«ä¸€è‡´ã™ã‚‹ã™ã¹ã¦ã®è¦ç´ 
scrapling extract fetch 'https://example.com' content.md --css-selector '#fromSkipToProducts' --no-headless
scrapling extract stealthy-fetch 'https://nopecha.com/demo/cloudflare' captchas.html --css-selector '#padded_content a' --solve-cloudflare
```

> [!NOTE]
> MCPã‚µãƒ¼ãƒãƒ¼ã‚„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–Web Scraping Shellãªã©ã€ä»–ã«ã‚‚å¤šãã®è¿½åŠ æ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ãŒã€ã“ã®ãƒšãƒ¼ã‚¸ã¯ç°¡æ½”ã«ä¿ã¡ãŸã„ã¨æ€ã„ã¾ã™ã€‚å®Œå…¨ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯[ã“ã¡ã‚‰](https://scrapling.readthedocs.io/en/latest/)ã‚’ã”è¦§ãã ã•ã„

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

Scraplingã¯å¼·åŠ›ã§ã‚ã‚‹ã ã‘ã§ãªãã€è¶…é«˜é€Ÿã§ã™ã€‚ä»¥ä¸‹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã€Scraplingã®ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä»–ã®äººæ°—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨æ¯”è¼ƒã—ã¦ã„ã¾ã™ã€‚

### ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºé€Ÿåº¦ãƒ†ã‚¹ãƒˆï¼ˆ5000å€‹ã®ãƒã‚¹ãƒˆã•ã‚ŒãŸè¦ç´ ï¼‰

| # |      ãƒ©ã‚¤ãƒ–ãƒ©ãƒª      | æ™‚é–“(ms) | vs Scrapling |
|---|:-----------------:|:---------:|:------------:|
| 1 |     Scrapling     |   2.02    |     1.0x     |
| 2 |   Parsel/Scrapy   |   2.04    |     1.01     |
| 3 |     Raw Lxml      |   2.54    |    1.257     |
| 4 |      PyQuery      |   24.17   |     ~12x     |
| 5 |    Selectolax     |   82.63   |     ~41x     |
| 6 |  MechanicalSoup   |  1549.71  |   ~767.1x    |
| 7 |   BS4 with Lxml   |  1584.31  |   ~784.3x    |
| 8 | BS4 with html5lib |  3391.91  |   ~1679.1x   |


### è¦ç´ é¡ä¼¼æ€§ã¨ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

Scraplingã®é©å¿œå‹è¦ç´ æ¤œç´¢æ©Ÿèƒ½ã¯ä»£æ›¿æ‰‹æ®µã‚’å¤§å¹…ã«ä¸Šå›ã‚Šã¾ã™ï¼š

| ãƒ©ã‚¤ãƒ–ãƒ©ãƒª     | æ™‚é–“(ms) | vs Scrapling |
|-------------|:---------:|:------------:|
| Scrapling   |   2.39    |     1.0x     |
| AutoScraper |   12.45   |    5.209x    |


> ã™ã¹ã¦ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯100å›ä»¥ä¸Šã®å®Ÿè¡Œã®å¹³å‡ã‚’è¡¨ã—ã¾ã™ã€‚æ–¹æ³•è«–ã«ã¤ã„ã¦ã¯[benchmarks.py](https://github.com/D4Vinci/Scrapling/blob/main/benchmarks.py)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

Scraplingã«ã¯Python 3.10ä»¥ä¸ŠãŒå¿…è¦ã§ã™ï¼š

```bash
pip install scrapling
```

ã“ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«ã¯ãƒ‘ãƒ¼ã‚µãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ã¨ãã®ä¾å­˜é–¢ä¿‚ã®ã¿ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€Fetcherã‚„ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ä¾å­˜é–¢ä¿‚ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚

### ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ä¾å­˜é–¢ä¿‚

1. ä»¥ä¸‹ã®è¿½åŠ æ©Ÿèƒ½ã€Fetcherã€ã¾ãŸã¯ãã‚Œã‚‰ã®ã‚¯ãƒ©ã‚¹ã®ã„ãšã‚Œã‹ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€Fetcherã®ä¾å­˜é–¢ä¿‚ã¨ãƒ–ãƒ©ã‚¦ã‚¶ã®ä¾å­˜é–¢ä¿‚ã‚’æ¬¡ã®ã‚ˆã†ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š
    ```bash
    pip install "scrapling[fetchers]"

    scrapling install           # normal install
    scrapling install  --force  # force reinstall
    ```

    ã“ã‚Œã«ã‚ˆã‚Šã€ã™ã¹ã¦ã®ãƒ–ãƒ©ã‚¦ã‚¶ã€ãŠã‚ˆã³ãã‚Œã‚‰ã®ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚ã¨fingerprintæ“ä½œä¾å­˜é–¢ä¿‚ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚

    ã¾ãŸã¯ã€ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ä»£ã‚ã‚Šã«ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼š
    ```python
    from scrapling.cli import install

    install([], standalone_mode=False)          # normal install
    install(["--force"], standalone_mode=False) # force reinstall
    ```

2. è¿½åŠ æ©Ÿèƒ½ï¼š
   - MCPã‚µãƒ¼ãƒãƒ¼æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼š
       ```bash
       pip install "scrapling[ai]"
       ```
   - Shellæ©Ÿèƒ½ï¼ˆWeb Scraping Shellã¨`extract`ã‚³ãƒãƒ³ãƒ‰ï¼‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼š
       ```bash
       pip install "scrapling[shell]"
       ```
   - ã™ã¹ã¦ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼š
       ```bash
       pip install "scrapling[all]"
       ```
   ã“ã‚Œã‚‰ã®è¿½åŠ æ©Ÿèƒ½ã®ã„ãšã‚Œã‹ã®å¾Œï¼ˆã¾ã ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ãªã„å ´åˆï¼‰ã€`scrapling install`ã§ãƒ–ãƒ©ã‚¦ã‚¶ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„

### Docker
DockerHubã‹ã‚‰æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§ã™ã¹ã¦ã®è¿½åŠ æ©Ÿèƒ½ã¨ãƒ–ãƒ©ã‚¦ã‚¶ã‚’å«ã‚€Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼š
```bash
docker pull pyd4vinci/scrapling
```
ã¾ãŸã¯GitHubãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼š
```bash
docker pull ghcr.io/d4vinci/scrapling:latest
```
ã“ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã¯ã€GitHub Actionsã¨ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ¡ã‚¤ãƒ³ãƒ–ãƒ©ãƒ³ãƒã‚’ä½¿ç”¨ã—ã¦è‡ªå‹•çš„ã«ãƒ“ãƒ«ãƒ‰ãŠã‚ˆã³ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚

## è²¢çŒ®

è²¢çŒ®ã‚’æ­“è¿ã—ã¾ã™ï¼å§‹ã‚ã‚‹å‰ã«[è²¢çŒ®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³](https://github.com/D4Vinci/Scrapling/blob/main/CONTRIBUTING.md)ã‚’ãŠèª­ã¿ãã ã•ã„ã€‚

## å…è²¬äº‹é …

> [!CAUTION]
> ã“ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯æ•™è‚²ãŠã‚ˆã³ç ”ç©¶ç›®çš„ã®ã¿ã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€åœ°åŸŸãŠã‚ˆã³å›½éš›çš„ãªãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŠã‚ˆã³ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼æ³•ã«æº–æ‹ ã™ã‚‹ã“ã¨ã«åŒæ„ã—ãŸã‚‚ã®ã¨ã¿ãªã•ã‚Œã¾ã™ã€‚è‘—è€…ãŠã‚ˆã³è²¢çŒ®è€…ã¯ã€ã“ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®èª¤ç”¨ã«ã¤ã„ã¦è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚å¸¸ã«ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã¨robots.txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’å°Šé‡ã—ã¦ãã ã•ã„ã€‚

## ğŸ“ å¼•ç”¨
ç ”ç©¶ç›®çš„ã§å½“ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã•ã‚ŒãŸå ´åˆã¯ã€ä»¥ä¸‹ã®å‚è€ƒæ–‡çŒ®ã§å¼•ç”¨ã—ã¦ãã ã•ã„ï¼š
```text
  @misc{scrapling,
    author = {Karim Shoair},
    title = {Scrapling},
    year = {2024},
    url = {https://github.com/D4Vinci/Scrapling},
    note = {An adaptive Web Scraping framework that handles everything from a single request to a full-scale crawl!}
  }
```

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ä½œå“ã¯BSD-3-Clauseãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## è¬è¾

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¯æ¬¡ã‹ã‚‰é©å¿œã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼š
- Parselï¼ˆBSDãƒ©ã‚¤ã‚»ãƒ³ã‚¹ï¼‰â€” [translator](https://github.com/D4Vinci/Scrapling/blob/main/scrapling/core/translator.py)ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ä½¿ç”¨

---
<div align="center"><small>Karim Shoairã«ã‚ˆã£ã¦â¤ï¸ã§ãƒ‡ã‚¶ã‚¤ãƒ³ãŠã‚ˆã³ä½œæˆã•ã‚Œã¾ã—ãŸã€‚</small></div><br>
